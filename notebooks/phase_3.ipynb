{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoRgZP9ZhnuVyqh40CpyDb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **1.Mount Drive**"],"metadata":{"id":"A6lL-T9VnwIz"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGIBm-KmnfE_","executionInfo":{"status":"ok","timestamp":1763028040108,"user_tz":-330,"elapsed":4957,"user":{"displayName":"Gumpu UshaSri","userId":"00966824892683506141"}},"outputId":"1114cf43-56ab-4e49-b407-ef3a57521fbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive mounted successfully.\n"]}],"source":["# =============================\n","# PHASE 3 — CELL 1: Mount Drive\n","# =============================\n","\n","import sys\n","\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"Drive mounted successfully.\")\n","else:\n","    print(\"Running locally — no Drive mount required.\")"]},{"cell_type":"markdown","source":["# **2.Install Required Libs**"],"metadata":{"id":"xo0zcPKVn_pw"}},{"cell_type":"code","source":["# ========================================\n","# PHASE 3 — CELL 2: Install Required Libs\n","# ========================================\n","\n","# Run this cell ONLY if you're in Google Colab.\n","# Skip on local machine if libs already installed.\n","\n","!pip install -q tldextract python-whois beautifulsoup4 textblob textstat scikit-learn joblib spacy\n","\n","# ensure spaCy model exists\n","import importlib, subprocess\n","try:\n","    importlib.import_module(\"en_core_web_sm\")\n","except:\n","    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n","\n","print(\"All required libraries installed & verified.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_8PtO5YGn0xr","executionInfo":{"status":"ok","timestamp":1763028081224,"user_tz":-330,"elapsed":41120,"user":{"displayName":"Gumpu UshaSri","userId":"00966824892683506141"}},"outputId":"01fe8d32-a4ff-45ab-ee8d-ba362be2e6ec"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["All required libraries installed & verified.\n"]}]},{"cell_type":"markdown","source":["# **3.Import Dependencies**"],"metadata":{"id":"eO1BI2yXoUI2"}},{"cell_type":"code","source":["# ====================================\n","# PHASE 3 — CELL 3: Import Dependencies\n","# ====================================\n","\n","import os\n","import re\n","import time\n","import joblib\n","import random\n","import requests\n","import tldextract\n","import datetime\n","import numpy as np\n","import pandas as pd\n","\n","from bs4 import BeautifulSoup\n","from textblob import TextBlob\n","import textstat\n","import spacy\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils import resample\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","print(\"Phase 3 dependencies successfully imported.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHeZ7DWVoJxm","executionInfo":{"status":"ok","timestamp":1763028085434,"user_tz":-330,"elapsed":4208,"user":{"displayName":"Gumpu UshaSri","userId":"00966824892683506141"}},"outputId":"03d517de-fa81-4bd3-ad65-445ab261102a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Phase 3 dependencies successfully imported.\n"]}]},{"cell_type":"markdown","source":["# **4.Full Retraining Code**"],"metadata":{"id":"4dZoYRfZookJ"}},{"cell_type":"code","source":["# =====================================\n","# PHASE 3 — CELL 4: FULL RETRAINING CODE\n","# =====================================\n","\n","# ---------------------------\n","# Step 1: Real website list\n","# ---------------------------\n","real_sites = [\n"," \"https://amazon.in\", \"https://amazon.com\", \"https://flipkart.com\", \"https://myntra.com\",\n"," \"https://ajio.com\", \"https://meesho.com\", \"https://nykaa.com\", \"https://paytm.com\",\n"," \"https://bigbasket.com\", \"https://reliancedigital.in\", \"https://snapdeal.com\",\n"," \"https://swiggy.com\", \"https://zomato.com\", \"https://ola.in\", \"https://uber.com\"\n","]\n","\n","\n","# ---------------------------\n","# Step 2: Fake website generator\n","# ---------------------------\n","fake_patterns = [\"-discount\", \"-sale\", \"-store\", \"-offer\", \"-deal\", \"-promo\"]\n","suspicious_tlds = [\"xyz\", \"shop\", \"online\", \"top\", \"buzz\"]\n","brands = [\"amazonn\", \"flipkarrt\", \"myntraa\", \"bestbuys\", \"shopkart\", \"superdeal\"]\n","\n","fake_sites = []\n","for i in range(300):\n","    b = random.choice(brands)\n","    p = random.choice(fake_patterns)\n","    t = random.choice(suspicious_tlds)\n","    fake_sites.append(f\"https://{b}{p}{random.randint(1,999)}.{t}\")\n","\n","\n","# ---------------------------\n","# Step 3: Text scraping\n","# ---------------------------\n","def fetch_text(url):\n","    try:\n","        headers = {\"User-Agent\": \"Mozilla/5.0 ScamScanBot\"}\n","        r = requests.get(url, timeout=6, headers=headers)\n","        soup = BeautifulSoup(r.text, \"html.parser\")\n","        return soup.get_text(separator=\" \")\n","    except:\n","        # fallback for synthetic fakes\n","        return \"LIMITED OFFER! Buy now and get 90% discount. Free shipping.\"\n","\n","\n","# ---------------------------\n","# Step 4: Feature extraction\n","# ---------------------------\n","def preprocess(text):\n","    if not text:\n","        return \"\"\n","    text = re.sub(r\"[^A-Za-z0-9\\s%-]\", \" \", text)\n","    text = text.lower()\n","    return re.sub(r\"\\s+\", \" \", text).strip()\n","\n","\n","def extract_features(url, raw):\n","    clean = preprocess(raw)\n","\n","    d = tldextract.extract(url)\n","\n","    return {\n","        \"url\": url,\n","        \"raw_text\": raw,\n","        \"clean_text\": clean,\n","        \"url_length\": len(url),\n","        \"num_digits\": sum(c.isdigit() for c in url),\n","        \"num_special_chars\": len(re.findall(r\"[\\W_]\", url)),\n","        \"has_https\": int(url.startswith(\"https\")),\n","        \"domain_len\": len(d.domain),\n","        \"suffix_len\": len(d.suffix),\n","        \"domain_age_days\": -1,   # WHOIS skipped to avoid rate limits\n","        \"text_length\": len(clean.split()),\n","        \"scam_keyword_score\": sum(k in clean for k in [\"discount\",\"offer\",\"free\",\"promo\",\"sale\"]),\n","        \"entity_count\": len(nlp(clean).ents) if clean else 0\n","    }\n","\n","\n","# ---------------------------\n","# Step 5: Build Dataset\n","# ---------------------------\n","records = []\n","\n","# REAL sites\n","for url in real_sites:\n","    raw = fetch_text(url)\n","    feat = extract_features(url, raw)\n","    feat[\"label\"] = 0\n","    records.append(feat)\n","    time.sleep(1)\n","\n","# FAKE sites\n","for url in fake_sites:\n","    raw = fetch_text(url)\n","    feat = extract_features(url, raw)\n","    feat[\"label\"] = 1\n","    records.append(feat)\n","\n","df = pd.DataFrame(records)\n","print(\"Dataset created:\", df.shape)\n","\n","\n","# ---------------------------\n","# Step 6: TF-IDF Vectorization\n","# ---------------------------\n","tfidf = TfidfVectorizer(max_features=500)\n","tfidf_matrix = tfidf.fit_transform(df[\"clean_text\"])\n","\n","\n","# ---------------------------\n","# Step 7: Final training matrix\n","# ---------------------------\n","numeric_cols = [\n","    \"url_length\", \"num_digits\", \"num_special_chars\", \"has_https\",\n","    \"domain_len\", \"suffix_len\", \"domain_age_days\",\n","    \"text_length\", \"scam_keyword_score\", \"entity_count\"\n","]\n","\n","X_num = df[numeric_cols].values\n","from scipy.sparse import hstack\n","X = hstack([X_num, tfidf_matrix])\n","y = df[\"label\"].values\n","\n","\n","# ---------------------------\n","# Step 8: Train-test split\n","# ---------------------------\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, random_state=42, stratify=y\n",")\n","\n","\n","# ---------------------------\n","# Step 9: Train RandomForest\n","# ---------------------------\n","rf = RandomForestClassifier(n_estimators=200, random_state=42)\n","rf.fit(X_train, y_train)\n","y_pred = rf.predict(X_test)\n","\n","\n","# ---------------------------\n","# Step 10: Metrics\n","# ---------------------------\n","print(\"\\n===== MODEL EVALUATION =====\")\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Precision:\", precision_score(y_test, y_pred))\n","print(\"Recall:\", recall_score(y_test, y_pred))\n","print(\"F1 Score:\", f1_score(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","\n","\n","# ---------------------------\n","# Step 11: Save Model + TF-IDF + Dataset\n","# ---------------------------\n","joblib.dump(rf, \"scamscan_model_retrained.pkl\")\n","joblib.dump(tfidf, \"tfidf_vectorizer_retrained.pkl\")\n","df.to_csv(\"df_final_retrained.csv\", index=False)\n","\n","print(\"\\nModels & dataset saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-osao8T4odqZ","executionInfo":{"status":"ok","timestamp":1763028235640,"user_tz":-330,"elapsed":150208,"user":{"displayName":"Gumpu UshaSri","userId":"00966824892683506141"}},"outputId":"d25a46bb-08cc-4851-8e3a-0304ff59c482"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset created: (315, 14)\n","\n","===== MODEL EVALUATION =====\n","Accuracy: 1.0\n","Precision: 1.0\n","Recall: 1.0\n","F1 Score: 1.0\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00         4\n","           1       1.00      1.00      1.00        75\n","\n","    accuracy                           1.00        79\n","   macro avg       1.00      1.00      1.00        79\n","weighted avg       1.00      1.00      1.00        79\n","\n","\n","Models & dataset saved successfully!\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"N9N8l-zRp-jd","executionInfo":{"status":"ok","timestamp":1763028236593,"user_tz":-330,"elapsed":27,"user":{"displayName":"Gumpu UshaSri","userId":"00966824892683506141"}}},"execution_count":5,"outputs":[]}]}